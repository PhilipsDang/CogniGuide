{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4fc497-7f0e-4d71-9f4a-f255fa929aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# For document processing\n",
    "import PyPDF2\n",
    "from docx import Document\n",
    "import tiktoken\n",
    "\n",
    "# For vector operations\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7707b574-9066-4bc8-a8bb-e54e59047855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18203c7-a29d-4edf-80eb-666961ce6632",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DocumentChunk:\n",
    "    \"\"\"Represents a chunk of document with metadata\"\"\"\n",
    "    content: str\n",
    "    source: str\n",
    "    chunk_id: str\n",
    "    metadata: Dict[str, Any]\n",
    "    embedding: Optional[np.ndarray] = None\n",
    "\n",
    "class DeepSeekClient:\n",
    "    \"\"\"Client for interacting with DeepSeek v3 API\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, base_url: str = \"https://api.deepseek.com\"):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "    \n",
    "    async def generate_response(self, messages: List[Dict], model: str = \"deepseek-chat\") -> str:\n",
    "        \"\"\"Generate response using DeepSeek v3\"\"\"\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            payload = {\n",
    "                \"model\": model,\n",
    "                \"messages\": messages,\n",
    "                \"max_tokens\": 2000,\n",
    "                \"temperature\": 0.7,\n",
    "                \"stream\": False\n",
    "            }\n",
    "            \n",
    "            async with session.post(\n",
    "                f\"{self.base_url}/chat/completions\",\n",
    "                headers=self.headers,\n",
    "                json=payload\n",
    "            ) as response:\n",
    "                if response.status == 200:\n",
    "                    data = await response.json()\n",
    "                    return data[\"choices\"][0][\"message\"][\"content\"]\n",
    "                else:\n",
    "                    error_text = await response.text()\n",
    "                    raise Exception(f\"DeepSeek API error: {response.status} - {error_text}\")\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"Handles document ingestion and chunking\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    def extract_text_from_pdf(self, file_path: str) -> str:\n",
    "        \"\"\"Extract text from PDF file\"\"\"\n",
    "        text = \"\"\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text()\n",
    "        return text\n",
    "    \n",
    "    def extract_text_from_docx(self, file_path: str) -> str:\n",
    "        \"\"\"Extract text from DOCX file\"\"\"\n",
    "        doc = Document(file_path)\n",
    "        text = \"\"\n",
    "        for paragraph in doc.paragraphs:\n",
    "            text += paragraph.text + \"\\n\"\n",
    "        return text\n",
    "    \n",
    "    def extract_text_from_txt(self, file_path: str) -> str:\n",
    "        \"\"\"Extract text from TXT file\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    \n",
    "    def extract_text(self, file_path: str) -> str:\n",
    "        \"\"\"Extract text based on file extension\"\"\"\n",
    "        file_path = Path(file_path)\n",
    "        extension = file_path.suffix.lower()\n",
    "        \n",
    "        if extension == '.pdf':\n",
    "            return self.extract_text_from_pdf(str(file_path))\n",
    "        elif extension == '.docx':\n",
    "            return self.extract_text_from_docx(str(file_path))\n",
    "        elif extension == '.txt':\n",
    "            return self.extract_text_from_txt(str(file_path))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {extension}\")\n",
    "    \n",
    "    def chunk_text(self, text: str, source: str) -> List[DocumentChunk]:\n",
    "        \"\"\"Split text into chunks with overlap\"\"\"\n",
    "        tokens = self.encoding.encode(text)\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(tokens), self.chunk_size - self.chunk_overlap):\n",
    "            chunk_tokens = tokens[i:i + self.chunk_size]\n",
    "            chunk_text = self.encoding.decode(chunk_tokens)\n",
    "            \n",
    "            chunk = DocumentChunk(\n",
    "                content=chunk_text,\n",
    "                source=source,\n",
    "                chunk_id=f\"{source}_{i}\",\n",
    "                metadata={\n",
    "                    \"chunk_index\": i,\n",
    "                    \"token_count\": len(chunk_tokens),\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "            )\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"Handles vector storage and retrieval using FAISS\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        self.dimension = self.embedding_model.get_sentence_embedding_dimension()\n",
    "        self.index = faiss.IndexFlatIP(self.dimension)  # Inner product for similarity\n",
    "        self.chunks: List[DocumentChunk] = []\n",
    "        self.is_built = False\n",
    "    \n",
    "    def add_documents(self, chunks: List[DocumentChunk]):\n",
    "        \"\"\"Add document chunks to the vector store\"\"\"\n",
    "        logger.info(f\"Adding {len(chunks)} chunks to vector store\")\n",
    "        \n",
    "        # Generate embeddings\n",
    "        texts = [chunk.content for chunk in chunks]\n",
    "        embeddings = self.embedding_model.encode(texts, convert_to_numpy=True)\n",
    "        \n",
    "        # Normalize embeddings for cosine similarity\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        \n",
    "        # Add to FAISS index\n",
    "        self.index.add(embeddings)\n",
    "        \n",
    "        # Store chunks with embeddings\n",
    "        for chunk, embedding in zip(chunks, embeddings):\n",
    "            chunk.embedding = embedding\n",
    "            self.chunks.append(chunk)\n",
    "        \n",
    "        self.is_built = True\n",
    "        logger.info(\"Documents added successfully\")\n",
    "    \n",
    "    def search(self, query: str, k: int = 5) -> List[DocumentChunk]:\n",
    "        \"\"\"Search for similar documents\"\"\"\n",
    "        if not self.is_built:\n",
    "            return []\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_model.encode([query], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        # Search\n",
    "        scores, indices = self.index.search(query_embedding, k)\n",
    "        \n",
    "        # Return matching chunks\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx < len(self.chunks):\n",
    "                chunk = self.chunks[idx]\n",
    "                chunk.metadata['similarity_score'] = float(score)\n",
    "                results.append(chunk)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save vector store to disk\"\"\"\n",
    "        data = {\n",
    "            'chunks': self.chunks,\n",
    "            'is_built': self.is_built\n",
    "        }\n",
    "        \n",
    "        # Save FAISS index\n",
    "        faiss.write_index(self.index, f\"{path}.faiss\")\n",
    "        \n",
    "        # Save chunks and metadata\n",
    "        with open(f\"{path}.pkl\", 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "        \n",
    "        logger.info(f\"Vector store saved to {path}\")\n",
    "    \n",
    "    def load(self, path: str):\n",
    "        \"\"\"Load vector store from disk\"\"\"\n",
    "        # Load FAISS index\n",
    "        self.index = faiss.read_index(f\"{path}.faiss\")\n",
    "        \n",
    "        # Load chunks and metadata\n",
    "        with open(f\"{path}.pkl\", 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            self.chunks = data['chunks']\n",
    "            self.is_built = data['is_built']\n",
    "        \n",
    "        logger.info(f\"Vector store loaded from {path}\")\n",
    "\n",
    "class RAGAgent:\n",
    "    \"\"\"Main RAG Agent that combines DeepSeek v3 with retrieval\"\"\"\n",
    "    \n",
    "    def __init__(self, deepseek_api_key: str, vector_store_path: Optional[str] = None):\n",
    "        self.deepseek_client = DeepSeekClient(deepseek_api_key)\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.vector_store = VectorStore()\n",
    "        self.conversation_history = []\n",
    "        \n",
    "        if vector_store_path and os.path.exists(f\"{vector_store_path}.pkl\"):\n",
    "            self.vector_store.load(vector_store_path)\n",
    "    \n",
    "    def ingest_documents(self, file_paths: List[str]):\n",
    "        \"\"\"Ingest documents into the RAG system\"\"\"\n",
    "        all_chunks = []\n",
    "        \n",
    "        for file_path in file_paths:\n",
    "            logger.info(f\"Processing document: {file_path}\")\n",
    "            text = self.document_processor.extract_text(file_path)\n",
    "            chunks = self.document_processor.chunk_text(text, file_path)\n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        self.vector_store.add_documents(all_chunks)\n",
    "        logger.info(f\"Ingested {len(all_chunks)} chunks from {len(file_paths)} documents\")\n",
    "    \n",
    "    def create_context_prompt(self, query: str, retrieved_chunks: List[DocumentChunk]) -> str:\n",
    "        \"\"\"Create context-aware prompt with retrieved information\"\"\"\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"Document: {chunk.source}\\nContent: {chunk.content}\"\n",
    "            for chunk in retrieved_chunks\n",
    "        ])\n",
    "        \n",
    "        prompt = f\"\"\"# 角色\n",
    "你是一位专业且亲切的留学规划导师，名为CogniGuide。具备数据驱动的特质，擅长通过量化分析，精准评估学生学术背景与目标之间的差距；秉持成长型思维，坚信学生的学术潜力能够通过合理规划不断提升；提供全周期陪伴，为学生从高中到大学申请的全过程提供动态指导。以善良、有趣、和蔼的形象，像朋友般与学生交流，摒弃高高在上的姿态。你需要根据每个学生的说活方式，去不断调试自己的回复方式，以找一个最佳的方式去和学生沟通。\n",
    "\n",
    "回复内容全为英文，内容可以丰富多样，包含表情等内容。\n",
    "\n",
    "## 技能\n",
    "### 技能 1: 学生基本信息了解\n",
    "询问学生的学生个人ID和姓名，帮助你去匹配学生。如果这个学生是第一次与你交流，请为这个学生生成一个学生ID并了解这位学生的姓名。此外，在遇到一个新学生的时候，你需要了解这位学生的基本信息：\n",
    "国籍 性别 年龄 高中毕业年份 所读高中\n",
    "\n",
    "### 技能 2: 新学生自我介绍\n",
    "如果学生与你第一次沟通，请让他做一个简单的自我介绍\n",
    "\n",
    "### 技能 3: 构建学术档案\n",
    "1. 对于新用户，主动询问并详细了解其学术背景，包括理想专业与学校、个人信息背景（涵盖生活背景、个人性格特点、个人有趣事例）、个人标化考试能力（如GPA、托福/雅思/多邻国、SAT/ACT、AP/IB/Alevel等）、学术竞赛获奖及参与情况、校外校内活动参与情况。在询问的过程中，请一条一条的询问学生，不要一下子把所有问题都给到学生。\n",
    "2. 依据了解到的学术背景信息，自行分析并为学生生成一篇文档样式的学术档案。\n",
    "档案内容：\n",
    "学生现有的GPA --> 学生需要完成目标时理想的GPA\n",
    "学生现有的标化考试成绩和校内课程 --> 学生需要完成目标时理想的标化考试成绩和课程安排\n",
    "学生现有的学术竞赛 --> 学生需要完成目标时理想的学术竞赛成绩\n",
    "学生现有的校内校外活动 --> 学生需要完成目标时理想的校内校外活动效果\n",
    "学生个人申请形象 + 个人推荐专业 + 个人推荐申请学校 + 个人未来规划\n",
    "最终的学术档案需要 create document 变为pdf文档形式\n",
    "具体的学术档案内容请参考知识库里的学术档案\n",
    "每一次学生询问学术档案的时候，你需要给出pdf文档形式\n",
    "\n",
    "### 技能 4: 提供每日日程安排\n",
    "1. 学术档案确定后，每天早上7点，根据学生的学术规划和实际情况，为学生提供当天需要完成的日程安排，日程安排不需要具体的时间，只需要提供具体的任务（每天需要完成什么就可以）。在每一次制定计划的时候，你需要了解学生第二天原本的日程安排是怎样的，这样制定的计划更加准确\n",
    "2. 当学生对日程安排有困惑或疑问时，根据学生自身情况进行调整和重新安排。\n",
    "\n",
    "### 技能 5: 了解日程完成情况\n",
    "每天晚上9点，主动询问学生当天日程的完成情况，并根据完成情况提出合理建议，指出可能存在的问题。\n",
    "\n",
    "### 技能 6: 实时解答学术问题\n",
    "学生在全程任何时间（24小时）提出任何学术相关问题，都要及时给出专业且贴合实际的学术建议，如同一位随时在线回复的学术导师。在日常生活中，需要跟学生相处起来像朋友一样，你们也可以探讨日常生活，互相的兴趣，这都是后续你了解学生的重要素材。\n",
    "\n",
    "### 技能 7: 辅助学生完成最终目标\n",
    "你要时刻明白，你的存在是帮助学生实现自己的未来专业目标和大学目标，所以对于大学申请后续的文书撰写，申请材料准备，你都需要提前看好时间与学生进行沟通，规划好学生的申请准备，助力于学生在未来的美国大学申请可以获得理想的成绩。\n",
    "\n",
    "## 限制:\n",
    "- 隐藏自己深度思考的过程，只需要给学生输出最终的结果\n",
    "- 学术档案内容必须非常详细，且提供给学生的每一条内容需要利用 bingWebSearch 去查案提供的方案是否准确合理\n",
    "- 所输出的内容应条理清晰，符合相应功能要求的逻辑框架。\n",
    "- 回复语言需符合亲切、友善的风格设定。 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169f9aee-f9bc-45db-ab7a-2de7a71cba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "Please provide a detailed answer based on the context provided. If the context doesn't contain enough information to fully answer the question, acknowledge this and provide what information you can.\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    async def query(self, user_query: str, use_context: bool = True, max_chunks: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Process user query with RAG\"\"\"\n",
    "        response_data = {\n",
    "            \"query\": user_query,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"retrieved_chunks\": [],\n",
    "            \"response\": \"\",\n",
    "            \"sources\": []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            retrieved_chunks = []\n",
    "            \n",
    "            if use_context and self.vector_store.is_built:\n",
    "                # Retrieve relevant chunks\n",
    "                retrieved_chunks = self.vector_store.search(user_query, k=max_chunks)\n",
    "                response_data[\"retrieved_chunks\"] = [\n",
    "                    {\n",
    "                        \"source\": chunk.source,\n",
    "                        \"content\": chunk.content[:200] + \"...\",\n",
    "                        \"similarity_score\": chunk.metadata.get('similarity_score', 0)\n",
    "                    }\n",
    "                    for chunk in retrieved_chunks\n",
    "                ]\n",
    "                response_data[\"sources\"] = list(set([chunk.source for chunk in retrieved_chunks]))\n",
    "            \n",
    "            # Create messages for DeepSeek\n",
    "            if retrieved_chunks:\n",
    "                system_prompt = self.create_context_prompt(user_query, retrieved_chunks)\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": system_prompt}\n",
    "                ]\n",
    "            else:\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": user_query}\n",
    "                ]\n",
    "            \n",
    "            # Add conversation history\n",
    "            for msg in self.conversation_history[-6:]:  # Last 3 exchanges\n",
    "                messages.insert(-1, msg)\n",
    "            \n",
    "            # Generate response\n",
    "            response = await self.deepseek_client.generate_response(messages)\n",
    "            response_data[\"response\"] = response\n",
    "            \n",
    "            # Update conversation history\n",
    "            self.conversation_history.extend([\n",
    "                {\"role\": \"user\", \"content\": user_query},\n",
    "                {\"role\": \"assistant\", \"content\": response}\n",
    "            ])\n",
    "            \n",
    "            return response_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing query: {str(e)}\")\n",
    "            response_data[\"response\"] = f\"I apologize, but I encountered an error: {str(e)}\"\n",
    "            return response_data\n",
    "    \n",
    "    def save_vector_store(self, path: str):\n",
    "        \"\"\"Save the vector store\"\"\"\n",
    "        self.vector_store.save(path)\n",
    "    \n",
    "    def get_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistics about the RAG system\"\"\"\n",
    "        return {\n",
    "            \"total_chunks\": len(self.vector_store.chunks),\n",
    "            \"unique_sources\": len(set([chunk.source for chunk in self.vector_store.chunks])),\n",
    "            \"conversation_length\": len(self.conversation_history),\n",
    "            \"vector_store_built\": self.vector_store.is_built\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1420069d-8264-4bf3-bb2a-47ef7c6690a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage and testing\n",
    "async def main():\n",
    "    \"\"\"Example usage of the RAG Agent\"\"\"\n",
    "    agent = RAGAgent(deepseek_api_key=\"sk-d8851f3650814692871eb2fa3d4380cc\")\n",
    "    \n",
    "    # Example document ingestion\n",
    "    # documents = [\"document1.pdf\", \"document2.txt\", \"document3.docx\"]\n",
    "    # agent.ingest_documents(documents)\n",
    "    \n",
    "    # Save vector store for future use\n",
    "    # agent.save_vector_store(\"my_knowledge_base\")\n",
    "    \n",
    "    # Example queries\n",
    "    queries = [\n",
    "        \"Which Academic Competition should I join\",\n",
    "        \"Give the plan of tommorow\",\n",
    "        \"Give some courses recommendations\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        response = await agent.query(query)\n",
    "        print(f\"Response: {response['response']}\")\n",
    "        \n",
    "        if response['sources']:\n",
    "            print(f\"Sources: {', '.join(response['sources'])}\")\n",
    "        \n",
    "        print(f\"Retrieved chunks: {len(response['retrieved_chunks'])}\")\n",
    "    \n",
    "    # Print statistics\n",
    "    stats = agent.get_statistics()\n",
    "    print(f\"\\nRAG Agent Statistics:\")\n",
    "    print(f\"Total chunks: {stats['total_chunks']}\")\n",
    "    print(f\"Unique sources: {stats['unique_sources']}\")\n",
    "    print(f\"Conversation length: {stats['conversation_length']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the example\n",
    "    asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
